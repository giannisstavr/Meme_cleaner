{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3b3da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pass 1: Calculating dataset statistics...\n",
      "Pass 1 Complete.\n",
      "Calculated Mean: tensor([0.4715, 0.4526, 0.4411])\n",
      "Calculated Std:  tensor([0.3132, 0.3114, 0.3194])\n",
      "\n",
      "Classes: ['meme', 'no_meme']\n",
      "Class to index mapping: {'meme': 0, 'no_meme': 1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms # input for augmentation, resizing etc\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image # For checking GIF handling if needed\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Global dependecies\n",
    "image_size = (224,224) #change depending on model\n",
    "batch_size = 64 # Adjust as per your GPU memory\n",
    "data_path = r\"C:\\Users\\tsili\\Documents\\Meme_cleaner\\dataset\"\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "#calculate dataset mean and std values for normalisation\n",
    "\n",
    "#Create a temporary dataset with minimal transforms (NO normalization)\n",
    "stats_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the entire dataset with these minimal transforms\n",
    "stats_dataset = ImageFolder(root=data_path, transform=stats_transforms)\n",
    "\n",
    "# Create a loader to iterate over the dataset for stats calculation\n",
    "# shuffle=False is fine here. num_workers can be set for speed.\n",
    "stats_loader = DataLoader(\n",
    "    stats_dataset, batch_size=batch_size, shuffle=False, num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "print(\"Starting Pass 1: Calculating dataset statistics...\")\n",
    "\n",
    "# Initialize accumulators\n",
    "channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
    "\n",
    "for data, _ in stats_loader:\n",
    "    # data shape: [B, C, H, W]\n",
    "    # Sum over all dimensions except the channel dimension (C)\n",
    "    channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "    channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "    num_batches += 1\n",
    "\n",
    "# Calculate final mean and std\n",
    "mean = channels_sum / num_batches\n",
    "std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "print(\"Pass 1 Complete.\")\n",
    "print(f\"Calculated Mean: {mean}\")\n",
    "print(f\"Calculated Std:  {std}\\n\")\n",
    "\n",
    "\n",
    "#Pipeline \n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "\n",
    "    #augmentation here-------------------------\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # p=0.5 is default, just being explicit\n",
    "    transforms.RandomRotation(15),\n",
    "    # ColorJitter is very effective\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    # RandomAffine can do rotation, translation, scaling, and shear all in one\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #---------------------------------------------------------\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist()) # Use the calculated stats\n",
    "\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #Image net stas, though it is best to calucate the mean and std of the dataset to then normalise\n",
    "\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist()) # Use the calculated stats\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#load dataset\n",
    "full_dataset =ImageFolder(root=data_path)\n",
    "\n",
    "# Check class to index mapping\n",
    "print(f\"Classes: {full_dataset.classes}\")\n",
    "print(f\"Class to index mapping: {full_dataset.class_to_idx}\")\n",
    "num_classes = len(full_dataset.classes) # Should be 2 for binary classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88d9710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10828\n",
      "Number of validation samples: 1354\n",
      "Number of test samples: 1354\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ImageFolder(root=data_path, transform=train_transforms)\n",
    "val_test_dataset = ImageFolder(root=data_path, transform=val_test_transforms)\n",
    "\n",
    "\n",
    "# Get the list of targets (labels) for stratification\n",
    "# .targets is an attribute of ImageFolder containing the label for each image\n",
    "targets = train_dataset.targets\n",
    "\n",
    "# Create a list of indices from 0 to len(dataset)-1\n",
    "indices = list(range(len(targets)))\n",
    "\n",
    "train_indices, temp_indices,_,_ = train_test_split (\n",
    "    indices,\n",
    "    targets,\n",
    "    test_size= 0.2,\n",
    "    random_state= 42,\n",
    "    stratify= targets\n",
    ")\n",
    "\n",
    "\n",
    "temp_targets = [targets[i] for i in temp_indices]\n",
    "val_indices, test_indicies, _, _= train_test_split(\n",
    "    temp_indices,\n",
    "    temp_targets,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_targets\n",
    ")\n",
    "\n",
    "#combine the list of indices with the transforms\n",
    "\n",
    "train_data= Subset(train_dataset, train_indices)\n",
    "val_data=Subset(val_test_dataset, val_indices)\n",
    "test_data = Subset(val_test_dataset, test_indicies)\n",
    "\n",
    "# --- Verification ---\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(val_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): Conv2dNormActivation(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Starting training with PyTorch Lightning...\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMisconfigurationException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 154\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# --- Start Training! ---\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# This single line replaces our entire manual for-loop.\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training with PyTorch Lightning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_pl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# --- Run the Final Test ---\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# After training, you can easily load the best model and run it on your test set.\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining finished. Running on test set with the best model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tsili\\Documents\\pytorch\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tsili\\Documents\\pytorch\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tsili\\Documents\\pytorch\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tsili\\Documents\\pytorch\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:962\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28mself\u001b[39m._callback_connector._attach_model_callbacks()\n\u001b[32m    960\u001b[39m \u001b[38;5;28mself\u001b[39m._callback_connector._attach_model_logging_functions()\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m \u001b[43m_verify_loop_configurations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    965\u001b[39m \u001b[38;5;66;03m# SET UP THE TRAINER\u001b[39;00m\n\u001b[32m    966\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    967\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: setting up strategy environment\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tsili\\Documents\\pytorch\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:36\u001b[39m, in \u001b[36m_verify_loop_configurations\u001b[39m\u001b[34m(trainer)\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnexpected: Trainer state fn must be set before validating loop configuration.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.state.fn == TrainerFn.FITTING:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[43m__verify_train_val_loop_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     __verify_manual_optimization_support(trainer, model)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m trainer.state.fn == TrainerFn.VALIDATING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tsili\\Documents\\pytorch\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:59\u001b[39m, in \u001b[36m__verify_train_val_loop_configuration\u001b[39m\u001b[34m(trainer, model)\u001b[39m\n\u001b[32m     57\u001b[39m has_optimizers = is_overridden(\u001b[33m\"\u001b[39m\u001b[33mconfigure_optimizers\u001b[39m\u001b[33m\"\u001b[39m, model)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_optimizers:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[32m     60\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     61\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# verify minimum validation requirements\u001b[39;00m\n\u001b[32m     65\u001b[39m has_val_loader = trainer.fit_loop.epoch_loop.val_loop._data_source.is_defined()\n",
      "\u001b[31mMisconfigurationException\u001b[39m: No `configure_optimizers()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader \n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning.callbacks import ModelCheckpoint,EarlyStopping\n",
    "import os \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # Often useful for this\n",
    "\n",
    "#Data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_data, batch_size=128, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "class MemeClassifier(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=0.001 ,num_classes=2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights.DEFAULT)\n",
    "\n",
    "        print(self.model)\n",
    "\n",
    "        #Freeze pre-trained layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad=False\n",
    "        \n",
    "        #Replace the backbone\n",
    "        num_ftrs = self.model.classifier[3].in_features\n",
    "        self.model.classifier[3] = nn.Linear(in_features=num_ftrs, out_features=self.hparams.num_classes)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.test_preds = []\n",
    "        self.test_labels = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Forward pass on the model\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def _calculate_metrics(self, batch):\n",
    "        \"Helper function to avoid code repetion\"\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "\n",
    "        _, preds =torch.max(outputs, 1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        return loss, acc, preds, labels\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"this is what happens for one batch of training data\"\n",
    "        loss,acc, _, _ = self._calculate_metrics(batch)\n",
    "        # In your MemeClassifier's training_step method\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc, _, _ = self._calculate_metrics(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc, preds, labels = self._calculate_metrics(batch)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "        self.test_preds.append(preds)\n",
    "        self.test_labels.append(labels)\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        # This hook is called automatically at the end of the test run\n",
    "\n",
    "        # Concatenate all the collected predictions and labels from each batch\n",
    "        all_preds = torch.cat(self.test_preds).cpu().numpy()\n",
    "        all_labels = torch.cat(self.test_labels).cpu().numpy()\n",
    "\n",
    "        # Get the class names from your dataset object\n",
    "        # For this example, we'll use the hardcoded names.\n",
    "        # IMPORTANT: Ensure this order matches your dataset's classes\n",
    "        class_names = ['meme', 'no meme']\n",
    "\n",
    "        # --- 1. Plot the Confusion Matrix (Code from before) ---\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names\n",
    "        )\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        plt.title('Confusion Matrix', fontsize=16)\n",
    "        plt.show() # Display the plot\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50) # Add a separator for clarity\n",
    "\n",
    "        # --- 2. NEW: Calculate and Print the Classification Report ---\n",
    "        print(\"Classification Report:\")\n",
    "        report = classification_report(\n",
    "            all_labels,\n",
    "            all_preds,\n",
    "            target_names=class_names\n",
    "        )\n",
    "        print(report)\n",
    "\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # Clear the lists for the next potential test run\n",
    "        self.test_preds.clear()\n",
    "        self.test_labels.clear()\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "#initialise the model\n",
    "model_pl = MemeClassifier(learning_rate=0.001, num_classes=2)\n",
    "\n",
    "#Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='my_checkpoints/', #change this\n",
    "    filename = 'meme-classifier-{epoch:02d}-{val_acc:.2f}', \n",
    "    save_top_k=1,                # Save only the best 1 model\n",
    "    mode='max'                  # The mode should be 'max' for accuracy\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_acc', # The metric to monitor\n",
    "    patience=5,        # How many epochs to wait for improvement before stopping\n",
    "    verbose=True,      # Print a message when stopping\n",
    "    mode='min'         # 'max' for accuracy, 'min' for loss\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",          # Automatically uses GPU or MPS if available\n",
    "    precision=\"16-mixed\",\n",
    "    max_epochs=100,               # The number of epochs to train for\n",
    "    callbacks=[checkpoint_callback,early_stopping_callback], # Add our checkpoint callback\n",
    "    logger=True                  # Enables logging (e.g., for TensorBoard)\n",
    ")\n",
    "\n",
    "# --- Start Training! ---\n",
    "# This single line replaces our entire manual for-loop.\n",
    "print(\"Starting training with PyTorch Lightning...\")\n",
    "trainer.fit(model=model_pl, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# --- Run the Final Test ---\n",
    "# After training, you can easily load the best model and run it on your test set.\n",
    "print(\"\\nTraining finished. Running on test set with the best model...\")\n",
    "trainer.test(model=model_pl,dataloaders=test_loader, ckpt_path='best') # 'best' loads the best checkpoint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2caf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_results.py\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the lightning_logs directory\n",
    "log_dir = \"lightning_logs\"\n",
    "# ---\n",
    "\n",
    "# --- Find the latest run and the metrics.csv file ---\n",
    "try:\n",
    "    # Get all subdirectories in lightning_logs (these are the 'version_X' folders)\n",
    "    run_dirs = [os.path.join(log_dir, d) for d in os.listdir(log_dir) if os.path.isdir(os.path.join(log_dir, d))]\n",
    "    # Find the most recently modified directory\n",
    "    latest_run_dir = max(run_dirs, key=os.path.getmtime)\n",
    "    metrics_file_path = os.path.join(latest_run_dir, \"metrics.csv\")\n",
    "    print(f\"Reading logs from: {metrics_file_path}\")\n",
    "except (ValueError, FileNotFoundError):\n",
    "    print(f\"Error: Could not find 'metrics.csv' in the '{log_dir}' directory.\")\n",
    "    print(\"Please ensure your training has run and created the log files.\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(metrics_file_path):\n",
    "    print(f\"Error: The directory '{latest_run_dir}' does not contain a 'metrics.csv' file.\")\n",
    "    exit()\n",
    "\n",
    "# --- Read the data using pandas ---\n",
    "df = pd.read_csv(metrics_file_path)\n",
    "\n",
    "# --- DEBUGGING STEP: Inspect the data ---\n",
    "# This helps us see the exact column names\n",
    "print(\"\\n--- First 5 rows of the metrics file ---\")\n",
    "print(df.head())\n",
    "print(\"\\n--- Available columns in the CSV ---\")\n",
    "print(df.columns.tolist())\n",
    "print(\"-------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- Prepare the Data for Plotting ---\n",
    "# The CSVLogger saves metrics at each step. Validation metrics like 'val_loss'\n",
    "# only have a value at the end of an epoch, and are 'NaN' otherwise.\n",
    "# We can filter the dataframe to get only the rows where validation happened.\n",
    "epoch_data = df[df['val_loss'].notna()].reset_index()\n",
    "\n",
    "if epoch_data.empty:\n",
    "    print(\"Error: No completed validation epochs found in the log file.\")\n",
    "    print(\"Please make sure you have run at least one full training epoch.\")\n",
    "    exit()\n",
    "\n",
    "# --- Create the Plots ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle(f'Performance for Run: {os.path.basename(latest_run_dir)}', fontsize=18)\n",
    "\n",
    "# Plot 1: Accuracy vs. Epochs\n",
    "# We check if the columns exist before trying to plot them\n",
    "if 'train_acc_epoch' in epoch_data.columns and 'val_acc' in epoch_data.columns:\n",
    "    ax1.plot(epoch_data['epoch'], epoch_data['train_acc_epoch'], label='Training Accuracy', marker='o', linestyle='--')\n",
    "    ax1.plot(epoch_data['epoch'], epoch_data['val_acc'], label='Validation Accuracy', marker='o')\n",
    "    ax1.set_title('Model Accuracy vs. Epochs', fontsize=14)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "else:\n",
    "    ax1.set_title(\"Accuracy data not found.\\nCheck CSV columns and logging.\", fontsize=14)\n",
    "\n",
    "# Plot 2: Loss vs. Epochs\n",
    "if 'train_loss_epoch' in epoch_data.columns and 'val_loss' in epoch_data.columns:\n",
    "    ax2.plot(epoch_data['epoch'], epoch_data['train_loss_epoch'], label='Training Loss', marker='o', linestyle='--')\n",
    "    ax2.plot(epoch_data['epoch'], epoch_data['val_loss'], label='Validation Loss', marker='o')\n",
    "    ax2.set_title('Model Loss vs. Epochs', fontsize=14)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "else:\n",
    "    ax2.set_title(\"Loss data not found.\\nCheck CSV columns and logging.\", fontsize=14)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
